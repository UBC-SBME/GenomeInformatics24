---
title: BMEG 424 Assignment 1
output:
  github_document:
    toc: true
    toc_depth: 4
---
```{r setup, incldue=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# BMEG 424 Assignment 1: Introduction
## Introduction:
### The Boring Stuff
Welcome to BMEG 424! We would like to begin by outlining some of the key rules and regulations you will have to follow for the assignments in this course:

- All assignments are to be completed individually or with one partner
- If you choose to work in pairs for an assignment, ONE of you should make a submission on canvas. In the submission you MUST report the name & student number of BOTH partners. 
- Each time you work with a new partner you will need to create a new GitHub repo for that assignment. If you choose to work with the same partner for multiple assignments (or by yourself) you should include all assignments completed by the same TEAM in a single repo.
- Late assignments will have 10% deducted for each day they are late (0 is recorded after 3 days late). Making changes to the files on your repo after the assignment has been submitted on Canvas will result in a late penalty unless an instructor has specifically asked you to change something.
- Each student will have to ensure their assignment files are knit correctly (see below for instructions) and legible on GitHub. Up to 20% of the assignment marks can be removed at the TA's discretion for incorrect knitting or formatting of assignment files.
- We are aware that some of you will choose to use large language models like ChatGPT to complete some portions of the assignment. You are free to use ChatGPT or other LLMs to help you on the assignment, *provided you cite the model, include your prompt, and indicate what portion was generated by AI* for each question where you received help.
- More generally, it is a requirement that you include any and all sources from where you received help (excluding your previously mentioned partner) or information for completing any question on the assignments. If you are unsure of when/how to cite your sources (including LLMs), please get in contact with the instructors.
  
If any of these rules are unclear please get in contact with the instructors. 

### The structure of the assignments in BMEG 424/524
Assignments will be split up into 3 sections: Introduction, Experiment and Analysis, and Discussion. The introduction section is for the instructors to give you information and context for the assignment you are about to complete, there will never be any points assigned to this section but it is still essential that you read it carefully. The 'Experiment' and Analysis section is where you will be doing the bulk of your computational work. Much like your laboratory classes there will be a protocol for you to follow for the generation, processing and preparation of genomics data. Most of your marks in this section will come from sharing the code you used to complete the assigned tasks and/or the results you generated from data, but there may also be some thinking questions you need to answer. The final discussion section will contain questions about the theoretical underpinning of the topics covered in the assignment. 

Everything in the assignments that are worth marks will be in the following format:


```{bash,eval=FALSE}
#?# 0. This is what a question looks like. (0 pts)
You'd put your answer here
```
If you are not sure that your assignment is done simply search for #?# and make sure you've filled everything out.

### Installing R and R Studio
We will largely do our analysis in R. R is most easily set up by downloading and installing RStudio (https://rstudio-education.github.io/hopr/starting.html). Ensure you have set up CRAN and Bioconductor (https://www.bioconductor.org/install/) properly. If you have any questions about setting up RStudio or Bioconductor please contact one of the instructors. Messing up your R install can be a major headache. 

### Getting set up on the server
For our course we will be using a server based on a Docker/Linux system. This will be the place where you will do your assignments and project. There are several things that you need to be aware while working on the server:

**The server has limited storage and computer power.** Therefore, you will be required to be mindful of the processes you run as well as the files you keep.
In general, files that are uncompressed (e.g. fastq, sam) should either be compressed or deleted (more on that later) to ensure we don't run out of space. If we do run out of space, no one will be able to write new data to disk, which will halt all progress until space clears up.

**If we find that we are running out of space on the server we may remove some marks from any students found to be hoarding data/space. If you are having trouble managing your files please contact one of the teaching staff.**

  - **Windows system:** 
      
      a. Install a terminal emulator like Putty (*https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html*). 

      b. This will allow you to do a SSH connection using the server's IP (orca1.bcgsc.ca) and your credentials. You can read the terminal emulators instructions for how to set up an SSH connection on your system & emulator.
  
  - **Linux/Unix system (Apple computer and Ubuntu):** 
      
      a. Open a terminal
      
      b. Type the following command: ssh username@orca1.bcgsc.ca 
      
      c. When prompted, type your password to complete the login 
      
Now that you have successfully joined the server, let's get you used to some basic commands.

### Submission:
Submit your assignment as a knitted RMarkdown document. You will push your knitted RMarkdown document to your github repository (one for each group). You will then submit the link, along with the names and student numbers of all students who worked on the assignment to the assignment 2 page on Canvas. Your assignment should be submtited, and your last commit should be made, before 11:59pm on the day of the deadline. Late assignments will will be deducted 10% per day late. Assignments will not be accepted after 3 days past the deadline.

## Experiment and Analysis:
### Part 1: Working on the command line
#### a. Basic commands
If you are already familiar with working on linux/unix based systems, then you may want to skip this section. There are no marks assigned until "Creating and moving Files".

The command line is a powerful tool which allows you to interact a computer without a graphical user interface (GUI). The majority of bioinformatics tools/software are run through the command line rather than a GUI. Before you are able to use these software you will have to know how perform basic tasks on a computer without a GUI, such as moving around the file system, creating files and directories, and editing/moving files.

The first thing you will need to know how to do is to move around the file system. In linux systems files are organized in directories (often called folders in Windows systems). The directories are organized in a tree, with the root directory being the top directory (denoted by "/"). As a user, you are always located in a single directory which is called your "working directory". 

You can see what directory you are in by typing `pwd` into the command line. 

```{bash, eval=FALSE}
#?# 1. Paste the output of pwd here (0 pts)
# Note: This question, along with all the others in Part 1, will not be graded. This section is here purely to get you up to speed. 
```
Each of you has a home directory on the server. You can see the path to your home directory by typing `echo $HOME` or `echo ~` into the command line. 

If you want to see the contents of your current directory you can type `ls` into the command line. Some files/directories will be hidden by default but can be seen by typing `ls -a`. 

```{bash, eval=FALSE}
#?# 2. Paste the output of ls -a here (0 pts)

```
You can create a directory by typing `mkdir <directory name>` into the command line. You can move into a directory by typing `cd <directory name>` into the command line. You can move up a directory by typing `cd ..` into the command line. 

```{bash, eval=FALSE}
#?# 3. Create a directory called "A1" and move into it. Paste the output of pwd here (0 pts)

```
```{bash, eval=FALSE}
#?# 4. Type a command which will list the contents of the parent directory of your current working directory (0 pts)

```
#### b. Creating and moving files
You can create a file by typing `touch <file name>` into the command line. Usually we denote file type by including a period followed by an extension at the end of the file name. For example, genomic sequencing data is often stored in the fastq file format, and therefore these sequence files are usually marked with *filename.fastq*. However, unlike on Windows systems the file extension doesn't actually do anything on linux systems, a fastq file named *filename.txt* is still a fastq file.

You can copy a file to a new location by typing `cp <file name> <destination>` into the command line. You will have to include either a relative (path from current directory) or absolute (path starting from root directory) in front of the file name and destination if they are not in the working directory. You can delete a file by typing `rm <file name>` into the command line. 

If you want to copy a file to a new location AND delete the original file in a single command you can type `mv <file name> <destination>` into the command line. 

### Part 2: Conda
#### a. Installing Conda
Conda is a package and environment manager (https://docs.conda.io/en/latest/). Conda is an excellent tool for managing computing environments.

In your home directory use wget to download the following file: https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh
Run the Anaconda3-2021.11-Linux-x86_64.sh bash script you just downloaded. Say yes to any prompts you get. Finish you installation by logging out and back into the server.

```{bash,eval=FALSE}
#?# 5. Once you've logged back into the server, paste the output of conda --version here (0.25 pts)

```
#### b. Creating your first conda environment
Conda environments are a great way to manage the software you use for different projects. You can create a new conda environment by typing `conda create -n <environment name> <list of packages>` into the command line. Usually you'll want to include python in your environment. 

You'll want to keep a seperate environment for each of your assignments so that software you install for one assignment doesn't interfere with software you install for another assignment. 

```{bash, eval=FALSE}
#?# 6. Create a directory to store your environments in your home directory. Initialize a conda environment *inside* that directory called A1. Paste the commands you used below (1 pts)


```
Usually conda installs things in the ~/.conda directory. You are installing them in a different directory so that it's easy for the TA to find it if something goes wrong. 

Now you'll want to activate your new environment. You can do so by typing `conda activate --prefix <path to environment>` into the command line. 

#### c. Installing software with conda
Todays task involves aligning some sequencing data to a reference genome and then performing variant calling on the aligned data. Variant calling is a process where we identify difference between the reference genome and the genome of the sample we sequenced. This is commonly done to identify mutations in patient genomes after whole genome sequencing or whole exome sequencing.

We will be using a tool called bowtie2 to align our data and a tool called bcftools to perform variant calling. We'll also need a few other tools to complete our analysis, the list is:

- fastQC (https://www.bioinformatics.babraham.ac.uk/projects/fastqc/): comprehensive quality control measures of sequencing data.
- bowtie2 (http://bowtie-bio.sourceforge.net/bowtie2/index.shtml): alignments of sequencing data.
- htop & screen: bash system tools to visualize the server capacity and to run commands in the background, respectively
- samtools (http://www.htslib.org/): manipulation of sequencing data.
  - mpileup: a part of samtools which is used for variant calling from sequencing data. Will be installed with samtools.
  - bcftools (http://www.htslib.org/): a part of samtools which is used for variant calling from sequencing data. Will be installed with samtools.

In order to install the tools we need for this course you will have to add two channels on top of the defaults channel that comes loaded with conda. You can do this by typing `conda config --add channels <channel name>` into the command line. The two channels you will need to add are bioconda and conda-forge. 

```{bash, eval=FALSE}
#?# 7. AFTER adding the appropriate channels type the command which will list all the channels you have added to conda (0.25 pts) and paste the output below. (0.25 pts)

```

You can install all of the necessary tools by typing `conda install <list of packages>` into the command line. It's generally easier to install all the packages at once rather than one at a time and allow conda to resolve any conflicts. 

```{bash, eval=FALSE}
#?# 8. Install all the tools listed above into your A1 environment using a single command. Paste the command you used below. (1 pts)


```

### Part 3: Pipelines
Pipelines are a useful way to organize and automate your analysis of data. They allow you to set up a series of steps in a predefined and predictable order. This may seem like a waste of time on these assignments where you are only analyzing a single or a handful of samples, but it will be essential when you are analyzing hundreds or thousands of samples.

#### a. Setting up a pipeline
For this course we will be using a pipeline manager called Snakemake (https://snakemake.readthedocs.io/en/stable/). Snakemake is a python based pipeline manager which will easily interface with conda and your various software tools installed thereby. Snakemake also handles many of the common issues in pipeline management like parallelization, dependency management, error handling, and run configuration/customization.

First we'll need to add snakemake to our conda environment. You can do this by typing `conda install -c bioconda snakemake` into the command line. 
 
First create a directory within your A1 directory called "pipeline". Then create a file called "Snakefile" within the pipeline directory. 

Snakemake uses a python based syntax to define the steps in your pipeline, these steps are called "rules". Each rule has a set of inputs, and a set of outputs. Rule are dependent on one another based on their inputs and outputs. For example, if rule A has an output which is the input for rule B, then rule B will not run until rule A has completed. Rules which are not dependent on one another will run in parallel. 

Create your Snakefile in the pipeline directory. Here is an example of the rule for running fastqc on your input data (which should now be in ~/A1/).
You can do a little magic to tell snakemake where the conda environment is. You can do this by adding the following line to the top of your Snakefile:

```{python, eval=FALSE}
conda_env: "/home/<username>/condaEnvs/A1"
```
Then when you run snakemake you'll have to include the flag `--use-conda` to tell snakemake to use the conda environment you specified. This works because you only need one conda environment for the whole pipeline, and snakemake will automatically activate the environment for each rule.

Here is some of the Snakefile (the rule for running fastqc and the rule for aligning the data) for you to begin with:
    
```{python, eval=FALSE}
# Snakefile
rule fastqc:
    input:
        "/path/to/data/{sample}_1.fastq.gz",
        "/path/to/data/{sample}_2.fastq.gz"
    output:
        "fastqc/{sample}_1_fastqc.html",
        "fastqc/{sample}_2_fastqc.html"
    shell:
        "fastqc {input} --outdir fastqc"

rule align:
    input:
        fastq1 = "/path/to/data/{sample}_1.fastq.gz",
        fastq2 = "/path/to/data/{sample}_2.fastq.gz",
    output:
        sam = "aligned/{sample}.sam"
    shell:
        "bowtie2 -x /projects/bmeg/indexes/hg38/hg38_bowtie2_index -1 {input.fastq1} -2 {input.fastq2} -S {output.sam}"

```

Before you start running your snakefile you will need to make sure it is set up correctly. You can type `snakemake -np` into the command line to check that your snakefile is set up correctly. Note this will only work if your snakefile is called "Snakefile" and is in your current working directory. 

Our pipeline will consist of the following steps:
- Quality control of the raw sequencing data (fastqc)
- Alignment of the sequencing data to the reference genome (bowtie2, paired-end using both forward and reverse reads and the hg38 genome)
- Sorting the aligned data (samtools)
- Variant calling (bcftools; installed with samtools)

It is up to you to implement the remainining steps 


```{python, eval=FALSE}
#?# 9. Fill in the rest of the Snakefile to include rules for sorting the aligned data, indexing the sorted data, and calling variants. (5 pts)
# Note: Inlcude the ENTIRE Snakefile in your submission. Not just the rules you added.


# Snakefile

```

You should also check to see that the dependency map of your snakefile is correct. Once you have confirmed that your snakefile is set up correctly you can type `snakemake --dag | dot -Tsvg > dag.svg` into the command line to generate a dependency map of your snakefile. You can view the dependency map by typing `eog dag.svg` into the command line. 

```{bash, eval=FALSE}
#?# 10. Include the dependency map of your Snakefile below using Rmarkdown syntax (1 pts)
# The correct Rmarkdown syntax is ![NameOfImg](path/to/dag.svg) 
```
![dag](dag.svg)

```{bash, eval=FALSE}
#?# 11. Explain what the dependency map is showing and whether or not you think it is correct. (1 pts)

```

Run your pipeline on the sequence data provided. You can do this by typing `snakemake --use-conda --cores=1 --resources mem_mb=4000` into the command line. You can use screen and htop to check the server usage and determine the correct number of resources to allocate (NOTE: If you don't pass any resources to snakemake it will use all available resources on the server, which is very inconsiderate to your classmates). **Please do not exceed 4GB of memory per job or a single core**, use less if you think it's necessary. 

```{bash, eval=FALSE}
#?# 12. Paste the output of snakemake here (0.25 pts)

```
### Part 4: Quality Control
#### a. Quality Control of raw sequencing data
Download the fastqc reports from the fastqc directory and include each of the graphs in your Rmarkdown file the same way you included the dependency map. 

```
#?# 13. Include the fastqc graphs from the QC on the forward read (read 1) file in your Rmarkdown file below this block. For each graph include a brief description of what the graph is showing and whether or not you think the data passed the quality control. (5 pts) 
# Please try to seperate your descriptions by including an text block between for description. 
```

![FirstGraph](fastqc_1_1.png)

```
Description for graph 1
```
... 

### b. Quality control of the alignment
For this section we will be using samtools to check the alignment of our data.

```
#?# 14. Use samtools flagstat to check the alignment rate of the sample you ran. Paste the output below (0.5 pts) and explain what the output means (1.5 pts)

```

### Part 5: Vizualization and Downstream Analysis
#### a. Setting up a reproducible analysis in R
Before you start analyzing the sequencing data, we will go over some essential methods for reprodicible analysis in R. Generally installing packages in RStudio is very easy, with packages being mantained on the CRAN repository. For example to install the common ggplot visualization package in RStudio:

```{r, eval=FALSE}
install.packages("ggplot2")
```

or to install from Bioconductor, you can use the BiocManager package:

```{r, eval=FALSE}
if (!requireNamespace("BiocManager", quietly = TRUE)) {
    install.packages("BiocManager")
}
BiocManager::install("<package-name>")
```
However RStudio does not generally save which versions of packages you had installed at the time of a particular analysis, so if you were trying to reproduce your analysis at a later date you might run into serious headaches either updating the code or trying to install (potentially hundreds of) outdated packages and dependencies. In order to solve this issue you can use package managers which work similarly to how conda managed our command line software. In this course we will be using the renv package. 

```{r, eval=FALSE}
install.packages("renv")
```

An R library is a directory on your computer where R stores all the packages you have installed. You can see where your system library is by typing `.libPaths()` into your R interpreter. Normally when you type `install.packages()` into your R interpreter you install (from a repository like CRAN or Bioconductor) into what's caled the system/global library. This is the equivalent of installing software directly onto your computer instead of within a conda environment in that the global library will be shared across all projects. This is a remarkably bad practice with regards to reproduciblity and project-based analysis. 

Once you have renv installed you can create a new library by typing `renv::init()` into your R interpreter. This will create a new library in your current working directory. You can then install packages into this library by typing `renv::install()` into your R interpreter. You can also install packages from a specific repository by typing `renv::install("package_name", repos = "https://cran.rstudio.com")` into your R interpreter. Because renv doesn't interfere with existing package management workflows you can still use `install.packages() ` and `BiocManager::install()` to install packages into your project library so long as you've initialized renv in your project.

#### b. Basics of R Analysis
For this section you will want to install tidyverse (https://www.tidyverse.org/), a collection of packages for data manipulation and visualization. We will need it for the ggplot2 package which it includes. 

#?# 15. Below is an R function for reading a VCF file and counting the number of occurrences of each unique ref-alt pair in all of the SNPs present in the VCF file. There are a few bugs in the code. Debug the function (2 pts) and add comments to explain what each line of code is doing (1 pts). 
```{r}
count_SNPs <- function(file_path) {
    vcf_data <- read.table(file_path, header = FALSE, stringsAsFactors = FALSE)
    header_names <- unlist(strsplit(header_line, "\t"))

    ref_alt_data <- vcf_data[c("REF", "ALT")]
    ref_alt_data <- ref_alt_data[apply(ref_alt_data, 1, function(x) all(nchar(x) == 1)), ]

    counts <- table(ref_alt_data$REF, ref_alt_data$ALT)

    result_df <- data.frame(
        REF = rep(rownames(counts), each = ncol(counts)),
        ALT = rep(colnames(counts), times = nrow(counts)),
        COUNT = as.vector(counts)
    )

    return(result_df)
}
```

#?# 16. Use the returned data frame to plot the ref-alt pairs for all SNPs as a bar plot (1 pts)
```
# Include the code you used to generate the plot in this block. When you knit your document the plot will be generated and displayed below.

```
#?# 17. Below is a plot which shows the distribution of variants across the genome, with peaks being colored by the depth of reads at that genomic locus. Recreate this plot using the data from your sample. (4 pts)
```{r}
# HINT: You'll want to start by looking at the documentation for the GenomicRanges and GenomicDistributions packages.
library(GenomicRanges)
library(GenomicDistributions)
```

![Chr1_VariantsvsReadDepth](A1_draft_files/figure-gfm/unnamed-chunk-21-1.png)

#### c. Interactively viewing our variants in IGV
You can download the IGV viewer from here: https://software.broadinstitute.org/software/igv/home. IGV is an interactive tool to visualize different data types of genetic information (e.g. bam, bed files). You will install this tool to your **local computer**. To visualize where the reads of our ChIP analysis mapped in the genome. 

Once you boot up IGV select your reference genome and load your VCF file. Zoom into this position: `chr1: 1,000,000-1,100,000`. 
```
#?# 17. Post a screenshot of your IGV window below (1pts)

```

## Discussion (6 pts)
#?# 18. Do you think the subset of sequence data you analyzed was from Whole Genome Sequencing or Whole Exome Sequencing? Justify your response with evidence. (4 pts)
```{r}

```

```

```

Assuming your snakemake run was supposed to process 1000 samples instead of 1 sample. 

```{bash, eval=FALSE}
#?# 19. How would you go about checking the quality of all the samples? (1 pts)

```


```{bash, eval=FALSE}
#?# 20. If the run crashed on sample 500, how would you go about restarting the run? (1 pts)

```
# Contributions
Please note here the team members and their contributions to this assignment.
